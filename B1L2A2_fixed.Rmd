---
title: "Untitled"
author: "Aron, Sergey, Shahin"
date: "2026-01-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Assignment 2. 

### Task 1

```{r t2_1, echo=FALSE}
#######################
#
# TASK 2 CODE
#
#######################
library(data.table)
library(tree)
library(knitr)

data <- fread('data/bank-full.csv', stringsAsFactors = FALSE)
data$duration <- NULL
categorical_cols <- c('job', 'marital', 'education', 'default', 'housing', 'loan',
                      'contact', 'month', 'poutcome', 'y')

for (col in categorical_cols) {
  data[[col]] <- factor(data[[col]], levels = unique(data[[col]]))
}

n=dim(data)[1]
set.seed(12345) 
id=sample(1:n, floor(n*0.4)) 
train=data[id,] 
id1=setdiff(1:n, id)
set.seed(12345) 
id2=sample(id1, floor(n*0.3)) 
valid=data[id2,]
id3=setdiff(id1,id2)
test=data[id3,]
```

After splitting the data into training/test/validation at the 40/30/30 ratio, we have `r nrow(train)`/`r nrow(valid)`/`r nrow(test)` rows in each respective dataset.

### Task 2

```{r t2_2, echo=FALSE}

# a) Decision tree with default settings
tree_default <- tree(y ~ ., data = train) 

# b) Smallest allowed node size = 7000
# (minsize = smallest node size eligible to split)
tree_minsize_7000 <- tree(
  y ~ ., data = train,
  control = tree.control(nobs = nrow(train), minsize = 7000)
)

# c) Minimum deviance = 0.0005
# mindev is a *fraction of root deviance* required to split a node
tree_mindev_0.0005 <- tree(
  y ~ ., data = train,
  control = tree.control(nobs = nrow(train), mindev = 0.0005)
)

misclass_rate_tree <- function(model, data) {
  pred <- predict(model, newdata = data, type = "class")
  mean(pred != data$y)
}

results <- data.frame(
  Model = c("Default", "minsize = 7000", "mindev = 0.0005"),
  Internal_splits = c(sum(tree_default$frame$var != "<leaf>"),
                       sum(tree_minsize_7000$frame$var != "<leaf>"),
                       sum(tree_mindev_0.0005$frame$var != "<leaf>")),
  Train_Misclass = c(misclass_rate_tree(tree_default, train),
                     misclass_rate_tree(tree_minsize_7000, train),
                     misclass_rate_tree(tree_mindev_0.0005, train)),
  Valid_Misclass = c(misclass_rate_tree(tree_default, valid),
                     misclass_rate_tree(tree_minsize_7000, valid),
                     misclass_rate_tree(tree_mindev_0.0005, valid))
)
kable(
  results,
  digits = 3,
  caption = "Train and validation misclassification rates for tree models"
)
# Number of internal (split) nodes = total nodes - leaves

# print(num_splits)

# par(mfrow = c(1, 3))
# 
# plot(tree_default); text(tree_default); title("Default Settings")
# plot(tree_minsize_7000); text(tree_minsize_7000); title("minsize = 7000")
# plot(tree_mindev_0.0005); text(tree_mindev_0.0005); title("mindev = 0.0005")
# 
# par(mfrow = c(1, 1))
```

- Default: uses reasonable default parameters, relatively low bias and low variance
- Min size 7000: same training and misclassification rates as the default tree, although one less split than the default tree - but it does not affect the misclassification rates as all the nodes on one side of the tree after the initial split on poutcome predict "no"
- Min dev = 0.0005: Very many leaves, low bias but the highest variance, clear signs of overfitting. We need to prune this tree.


### Task 3

```{r t2_3, echo=FALSE}

seq_train <- prune.tree(tree_mindev_0.0005, newdata = train, method = "deviance")  
seq_valid <- prune.tree(tree_mindev_0.0005, newdata = valid, method = "deviance")

train_tot <- if (!is.null(seq_train$dev)) seq_train$dev else seq_train$deviance
valid_tot <- if (!is.null(seq_valid$dev)) seq_valid$dev else seq_valid$deviance

train_df <- data.frame(leaves = seq_train$size,
                       train_mean_dev = train_tot / nrow(train))
valid_df <- data.frame(leaves = seq_valid$size,
                       valid_mean_dev = valid_tot / nrow(valid))

dev_df <- merge(train_df, valid_df, by = "leaves", all = TRUE)
dev_df <- dev_df[dev_df$leaves <= 50, ]
dev_df <- dev_df[order(dev_df$leaves), ]

# Plot
plot(dev_df$leaves, dev_df$train_mean_dev, type = "l", lwd = 2, col = "blue",
     xlab = "Number of leaves", ylab = "Mean deviance",
     ylim = range(c(dev_df$train_mean_dev, dev_df$valid_mean_dev), na.rm = TRUE))

lines(dev_df$leaves, dev_df$valid_mean_dev, lwd = 2, col = "red")

legend("topright", legend = c("Train", "Validation"),
       col = c("blue", "red"), lwd = 2, lty = c(1, 1), bty = "n")

best_L <- dev_df$leaves[which.min(dev_df$valid_mean_dev)]
cat("Optimal number of leaves: ", best_L)
best_tree <- prune.tree(tree_mindev_0.0005, best = best_L)
```

As the number of leaves increases, the training deviance decreases monotonically, while the validation deviance initially decreases and then increases again. For very small trees (few leaves), both training and validation deviances are high, indicating underfitting and high bias. As we allow more leaves, the model becomes more flexible, bias decreases, and validation deviance improves, reaching its minimum at `r best_L` leaves. Beyond `r best_L` leaves, further increases in tree size continue to reduce training deviance but lead to higher validation deviance, which indicates that additional complexity mainly increases variance and causes overfitting. Thus a tree with `r best_L` leaves provides the best bias–variance tradeoff.


```{r t2_3_1, echo=FALSE}
frame <- best_tree$frame

# node ids are stored as rownames (character); convert once
node_id <- rownames(frame)
dev_by_node <- setNames(frame$dev, node_id)
var_by_node <- setNames(as.character(frame$var), node_id)
internal_nodes <- node_id[var_by_node != "<leaf>"]
split_vars <- unique(var_by_node[var_by_node != "<leaf>"])
imp <- setNames(rep(0, length(split_vars)), split_vars)

for (nid in internal_nodes) {
  node_num <- as.integer(nid)

  left_id  <- as.character(2 * node_num)
  right_id <- as.character(2 * node_num + 1)

  # If pruned, a child might not exist; skip defensively
  if (!(left_id %in% node_id) || !(right_id %in% node_id)) next

  delta <- dev_by_node[nid] - dev_by_node[left_id] - dev_by_node[right_id]  # deviance reduction
  v <- var_by_node[nid]
  imp[v] <- imp[v] + delta
}

sort(imp, decreasing = TRUE)
```

The above variable importance numbers are based on how much each variable reduces the node loss (impurity / deviance) over all the splits where it is used. From these numbers, it appears that poutcome (outcome of previous marketing campaign) and month (month of contact) play the most important role in decision making within the tree. Contact communication type and customer characteristics play a smaller role.

```{r t2_3_2, echo=FALSE}
models <- list(
  "CP 0.0005"      = tree_mindev_0.0005,
  "Optimal" = best_tree
)

misclass_df <- do.call(rbind, lapply(models, function(m) {
  c(
    Train = misclass_rate_tree(m, train),
    Valid = misclass_rate_tree(m, valid)
  )
}))

misclass_df <- data.frame(
  Model = rownames(misclass_df),
  Train_Misclass = misclass_df[, "Train"],
  Valid_Misclass = misclass_df[, "Valid"],
  row.names = NULL
)

kable(
  misclass_df,
  digits = 3,
  caption = "Train and validation misclassification rates tree model before and after pruning"
)
```
The `r best_L`‑leaf tree chosen by minimum validation deviance has essentially the same validation misclassification rate as the original tree before pruning. However, the pruned tree achieves a lower validation deviance, indicating that its predicted probabilities are better calibrated and capture more nuanced structure in the data. In terms of the bias–variance tradeoff, it reduces bias (better fit) without noticeably reducing 0–1 error on the validation set.

```{r t2_3_3, echo=FALSE}
# Plot the selected/pruned tree from the {tree} package
par(xpd = NA , mar = c(1.5, 1.5, 3, 1.5))
plot(best_tree, type = "uniform")                                   
text(best_tree, pretty = 0, cex=0.7)            
title(main = sprintf("Pruned tree (leaves = %d)", sum(best_tree$frame$var == "<leaf>")))
```

Key findings:

- Previous campaign success is the single most informative variable, essentially splitting the population into a low‑propensity group and a high‑propensity group.

- Timing matters: both within the non‑success and success groups, the specific month of contact has substantial influence, suggesting seasonality or within‑month effects in customer responsiveness.

- Customer profile variables such as job and balance play a secondary, fine‑tuning role: they help pick out especially promising or unpromising subsegments but do not overturn the primary pattern driven by past outcome and timing.

### Task 4

```{r t2_4, echo=FALSE}

format_cm <- function(cm) {
  # positive class is "yes" and negative is "no"
  tp <- cm["yes", "yes"]
  fn <- cm["yes", "no"]
  fp <- cm["no",  "yes"]
  tn <- cm["no",  "no"]

  cm_labeled <- matrix(
    c(
      paste0(tp, " (TP)"),
      paste0(fn, " (FN)"),
      paste0(fp, " (FP)"),
      paste0(tn, " (TN)")
    ),
    nrow = 2, byrow = TRUE,
    dimnames = list(
      Observed  = c("Observed yes", "Observed no"),
      Predicted = c("Predicted yes", "Predicted no")
    )
  )
  return(cm_labeled)
}

pred_test <- predict(best_tree, newdata = test, type = "class")


cm <- table(Observed = test$y, Predicted = pred_test)
cm_labeled <- format_cm(cm)
kable(
  cm_labeled,
  caption = "Confusion matrix for optimal tree on test data (TP = true positives, FP = false positives, FN = false negatives, TN = true negatives)"
)

calculate_metrics <- function(pred_test, test, cm) {
  accuracy <- mean(pred_test == test$y)
  print(paste("Accuracy:", accuracy))

  positive <- "yes"

  tp <- cm[positive, positive]
  fp <- sum(cm[, positive]) - tp
  fn <- sum(cm[positive, ]) - tp

  precision <- tp / (tp + fp)
  recall    <- tp / (tp + fn)
  f1        <- 2 * precision * recall / (precision + recall)

  print(paste("Precision:", precision))
  print(paste("Recall:", recall))
  print(paste("F1:", f1))
  
  list(
    accuracy  = accuracy,
    precision = precision,
    recall    = recall,
    f1        = f1
  )
}
metrics <- calculate_metrics(pred_test, test, cm)
```
From the numbers above we can see that the accuracy is about `r metrics$accuracy`, but the majority class (“no”) already accounts for about 90% of the data, so a trivial classifier that always predicts “no” would get almost the same accuracy. At the same time, precision for “yes” is `r metrics$precision` and recall is `r metrics$recall`, giving F1 around `r metrics$f1`. That means that the model:

- Misses most actual “yes” cases (low recall).
- Among predicted “yes”, a substantial fraction are wrong (precision moderate, not high).

So in terms of identifying the minority class, the predictive power is modest: the model finds only about one fifth of the true positives while still making a fair number of false alarms.

The F1 appears to be more informative here as the class distribution is highly skewed toward “no”, so accuracy is dominated by correct “no” predictions and hides the poor detection of “yes”. F1 focuses on the minority/positive class by combining precision and recall, so it better reflects the model’s usefulness for finding subscribers.

Therefore despite a high overall accuracy driven by the majority “no” class, the model’s performance on the important “yes” class is relatively weak, and F1 is a more appropriate performance summary than accuracy given the strong class imbalance.

### Task 5

```{r t2_5, echo=FALSE}
loss_mat <- matrix(
  c(0, 1,
    5, 0),
  nrow = 2, byrow = TRUE,
  dimnames = list(
    Observed  = c("no", "yes"),
    Predicted = c("no", "yes")
  )
)
pred_test_prob <- predict(best_tree, newdata = test, type = "vector")
exp_loss <- pred_test_prob %*% loss_mat
pred_cost <- colnames(exp_loss)[max.col(-exp_loss, ties.method = "first")]
pred_cost <- factor(pred_cost, levels = colnames(loss_mat))
cm <- table(Observed = test$y, Predicted = pred_cost)
cm_labeled <- format_cm(cm)
kable(
  cm_labeled,
  caption = "Confusion matrix for optimal tree on test data using custom loss matrix"
)
metrics_custom_loss <- calculate_metrics(pred_cost, test, cm)
```
As could be expected, when using a custom loss function that penalizes false negatives more harshly than false positives, we are seeing fewer false negatives than before. It results in a higher recall of the model and a higher F1 score at the expense of lower precision and accuracy.

### Task 6

```{r t2_6, echo=FALSE}
library(dplyr)

prob_yes_tree <- pred_test_prob[, "yes"]

# Fit logistic regression (binomial GLM with logit link)
logit_fit <- glm(
  y ~ .,
  data   = train,
  family = binomial(link = "logit")
)

# summary(logit_fit)
prob_yes_logit <- predict(logit_fit, newdata = test, type = "response")

pi_grid <- seq(0.05, 0.95, by = 0.05)

metrics_at_threshold <- function(thresh, y_true, p_yes) {
  pred_yes <- p_yes > thresh
  y_pos    <- y_true == "yes"
  y_neg    <- y_true == "no"

  tp <- sum(pred_yes & y_pos)
  fp <- sum(pred_yes & y_neg)
  fn <- sum(!pred_yes & y_pos)
  tn <- sum(!pred_yes & y_neg)

  tpr <- ifelse(tp + fn == 0, NA, tp / (tp + fn)) # sensitivity/recall
  fpr <- ifelse(fp + tn == 0, NA, fp / (fp + tn))

  c(threshold = thresh, TPR = tpr, FPR = fpr)
}

roc_tree <- t(sapply(pi_grid, metrics_at_threshold,
                     y_true = test$y, p_yes = prob_yes_tree))
roc_tree <- as.data.frame(roc_tree)


# ROC data for logistic regression
roc_logit <- as.data.frame(
  t(sapply(pi_grid, metrics_at_threshold,
           y_true = test$y, p_yes = prob_yes_logit))
)

plot(roc_tree$FPR, roc_tree$TPR, type = "b", pch = 19, col = "blue",
     xlab = "False Positive Rate (FPR)",
     ylab = "True Positive Rate (TPR)",
     main = "ROC curves: optimal tree vs logistic regression",
     xlim = c(0, 1), ylim = c(0, 1))

lines(roc_logit$FPR, roc_logit$TPR, type = "b", pch = 17, col = "red")

abline(0, 1, lty = 2, col = "grey")
legend("bottomright",
       legend = c("Tree (optimal)", "Logistic regression"),
       col    = c("blue", "red"),
       pch    = c(19, 17),
       bty    = "n")
```


The ROC plot shows that both models are clearly better than random (their curves lie well above the diagonal) and achieve similar ratios of FPR to TPR for most probability thresholds, with the tree model lying slightly above the logistic regression model and therefore slightly outperforming it for some probability thresholds.

At the same time, it should be noted that ROC curves treat TPR and FPR symmetrically and do not depend on class prevalence, so a model can look good in ROC space even when the positive class is very rare and performance on that class is mediocre. By contrast, precision–recall (PR) curves focus only on the positive class with the recall being equal to the TPR and the precision showing the fraction of predicted “yes” that are truly “yes”.

In our case, where we observe a significant imbalance between "yes" and "no" responses (with "no" responses accounting for about 89% of the data), the precision-recall curve is indeed a better choice than relying on ROC alone as it will give a clearer view of how well each model retrieves the “yes” class at acceptable false‑alert rates.

```{r t2_6_1, echo=FALSE}
pr_at_threshold <- function(thresh, y_true, p_yes) {
  pred_yes <- p_yes > thresh
  y_pos    <- y_true == "yes"
  y_neg    <- y_true == "no"

  tp <- sum(pred_yes & y_pos)
  fp <- sum(pred_yes & y_neg)
  fn <- sum(!pred_yes & y_pos)

  precision <- ifelse(tp + fp == 0, NA, tp / (tp + fp))
  recall    <- ifelse(tp + fn == 0, NA, tp / (tp + fn))  # = TPR

  c(threshold = thresh, Precision = precision, Recall = recall)
}


pr_tree <- as.data.frame(
  t(sapply(pi_grid, pr_at_threshold,
           y_true = test$y, p_yes = prob_yes_tree))
)

pr_logit <- as.data.frame(
  t(sapply(pi_grid, pr_at_threshold,
           y_true = test$y, p_yes = prob_yes_logit))
)

plot(pr_tree$Recall, pr_tree$Precision, type = "b", pch = 19, col = "blue",
     xlab = "Recall",
     ylab = "Precision",
     xlim = c(0, 1), ylim = c(0, 1),
     main = "Precision–Recall curves: optimal tree vs logistic regression")

lines(pr_logit$Recall, pr_logit$Precision, type = "b", pch = 17, col = "red")

legend("topright",
       legend = c("Tree (optimal)", "Logistic regression"),
       col    = c("blue", "red"),
       pch    = c(19, 17),
       bty    = "n")

text(pr_tree$Recall, pr_tree$Precision,
     labels = round(pr_tree$threshold, 2),
     pos = 4, cex = 0.6, col = "blue")

text(pr_logit$Recall, pr_logit$Precision,
     labels = round(pr_logit$threshold, 2),
     pos = 4, cex = 0.6, col = "red")
```
The precision-recall curve shown above demonstrates that by lowering the cutoff probability, we can achieve higher recall at the expense of lower precision and vice versa depending on the business goals. In this particular case, we can to expect the bank to prioritize higher recall as the cost of a missed business opportunity (a false negative) is significantly higher than the cost of an operator making a few additional phone calls that don't result in a new subscription (a false positive).

## Appendix


```{r, ref.label=setdiff(knitr::all_labels(), c('setup')), echo=TRUE, eval=FALSE}



```
