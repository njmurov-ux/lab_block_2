---
title: "lab02_assignment1"
output: html_document
date: "2025-11-25"
---

```{r setup, include=FALSE}

#install.packages("randomForest")
library(randomForest)
library(dplyr)

```

## Theory 
Question 1
In an ensemble model, is it true that the larger the number B of ensemble members
the more flexible the ensemble model?

No, the bias stays constant, atleast for numerical Y. The model does not fit the data better, bagging is a variance reduction techinque aimed to reduce the variance stemming from the sampling variance, by averaging over multiple simulated bootstrap samples. Hence the flexibility isn't increased. 


## Assignment 1

```{r echo = FALSE}

# Simulating missclassification error against the same Y-labels for 1, 10 and 100 trees. 
# Computing the spread and location of the missclassification error for 1000 simulations.
random_forest_sim <- function(NS = 25, condition = function(x1, x2){
  x1 < x2
  
}) {
  
  # ------------------------
  
  set.seed(1234)
  x1<-runif(1000)
  x2<-runif(1000)
  tedata<-cbind(x1,x2)
  y<-as.numeric(condition(x1, x2))
  telabels<-as.factor(y)
  
  
  
  # ------------------------
  
  
  ntree <- c(1, 10, 100)
  n <- 1000
  
  miss_df <- matrix(0, 3, length(ntree)) %>%  as.data.frame()
  rownames(miss_df) <- c("mu", "var", "sd")
  colnames(miss_df) <- ntree
  
  
  for (nt in ntree) {
    missrate <- rep(1, n)
    for (i in 1:n) {
      
      x1<-runif(100)
      x2<-runif(100)
      trdata<-cbind(x1,x2)
      y<-as.numeric(condition(x1, x2))
      trlabels<-as.factor(y)
      
      model <- randomForest(x = trdata, y = trlabels, ntree = nt, keep.forest = TRUE, 
                            nodesize = NS)
      
      y_hat <- predict(model, tedata)
      missrate[i] <-  mean(y_hat != telabels)
      
    }
    
    miss_df[1, which(nt == ntree)] <- mean(missrate)
    miss_df[2, which(nt == ntree)] <- var(missrate)
    miss_df[3, which(nt == ntree)] <- sd(missrate)
  }
  
  return(miss_df)
}

```

# Simulating for 3 different conditions
The condition for Y is x1 < x2.
```{r echo = FALSE}


# x1 < x2
rf1 <- random_forest_sim(NS = 25)

rf1 %>% round(4)

```


The condition for Y is x1 < 0.5.
```{r echo = FALSE}

# x1 < 0.5
rf2 <- random_forest_sim(condition = function(x1, x2){
  x1 < 0.5
})

rf2 %>% round(4)


```

The condition for Y is divide the domain into 4 equal squares, top right and bottom left is 1, otherwise 0.
```{r echo = FALSE}
# divide the domain into 4 equal squares, top right and bottom left is 1, otherwise 0.
rf3 <- random_forest_sim(NS = 12, condition = function(x1, x2){
  (x1<0.5 & x2<0.5) | (x1>0.5 & x2>0.5)
})

rf3 %>% round(4)


```


**What happens with the mean error rate when the number of trees in the random
forest grows? Why?**
For numerical Y, the bias drops when the number of bagged learners combined are increased. For this case we use majority voting. Say X shows wether the prediction from a tree is accurate, $X = {0, 1}$, X is ber(p) and $X_i$ is i.i.d for different trees. The sum of $X_i$ is bin(n, p). $$P(\hat{y} = Y = 1) = P(bin(n, p) > n/2) \approx P(N(np, np(1-p) > n/2) = \phi(\frac{\frac{n}{2} -np}{\sqrt{np(1-p)}})$$ Which scales as $\sqrt{n}\cdot c$. The limits of phi is 0 and 1 as it is a cdf. Meaning the model will become very confident in predictions (0 if p<0.5, 1 otherwise). So for a p = 0.51, the model would always predict 1 if n is large enough, for 1 tree the variance of $\hat{y}$ would be larger. This is why the error rate drops, out model becomes better at confidently predicting y for p close to 0.5. But the bias doesn't stay constant like for numerical Y, say Y = 1, for one tree $Bias = p - Y$, if p >0.5, our tree is better than a random guess, then the bias drops, if our tree is worse than a random guess, the bias actually increases! 



  
**The third dataset represents a slightly more complicated classification problem
than the first one. Still, you should get better performance for it when using sufficient trees in the random forest. Explain why you get better performance.**
\newline

This still holds for the same nodesize (12) on both.
```{r echo = FALSE}


print("with the others nodesize")
rf1 <- random_forest_sim(NS = 12)
rf1 %>% round(4)
rf3 <- random_forest_sim(NS = 12, condition = function(x1, x2){
  (x1<0.5 & x2<0.5) | (x1>0.5 & x2>0.5)
})
rf3 %>% round(4)


```

The reason for this is that lines parallell to the variable axis are drawn when a deciscion tree is trained, but in the first dataset the decision boundry is y=x, trees can still approximate this, but it requires more trees or depth, it will look like a stepwise function. 
