---
title: "lab_block_2"
author: "Aron, Sergey, Shahin"
date: "2025-11-14"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Assignment 1. 

# Assignment 2. Mixture models

## Task description

We are given three vectors of probabilities that defines the data-generating distribution for the cluster.

```{r task_2_mixture_model}
# TASK 2 CODES

# --- Data Generation (with cluster tracking) ---
seed_value <- 1234567890
set.seed(seed_value)
n=1000 
D=10 
x <- matrix(nrow=n, ncol=D)
true_pi=c(1/3, 1/3, 1/3)
true_mu <- matrix(nrow=3, ncol=D)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
points(true_mu[3,], type="o", col="green")
```


Every single data point is a vector consisting of 10 separate binary features. Each feature can only be a 0 or a 1, representing the absence or presence of an attribute. While a K-Means centroid is the average position of points in a cluster, the mu vector in a Bernoulli mixture model is the probability of each feature being "1" (or "yes") for a member of that cluster. The core difference is that Bernoulli centroids define the probability of features, while Gaussian centroids define the location of the cluster's center. 

After generating the training data, We can compress the original 10 dimensions to 2 using PCA (Principal Component Analysis) to demonstrate that it indeed forms 3 distinct clusters:

```{r task_2_training_data_generation}
# This vector will store the true cluster for each point
true_cluster_ids <- vector(length = n)

# Generate the training data
for(i in 1:n) {
  # Sample a cluster for the data point
  m <- sample(1:3, 1, prob = true_pi)
  # Store the cluster id
  true_cluster_ids[i] <- m
  # Generate the data point from the chosen cluster's distribution
  for(d in 1:D) {
    x[i,d] <- rbinom(1, 1, true_mu[m,d])
  }
}

# Plotting the Training Data with PCA
pca_result <- prcomp(x, center = TRUE, scale. = TRUE)
pca_plot_data <- as.data.frame(pca_result$x[, 1:2])
colnames(pca_plot_data) <- c("PC1", "PC2")
cluster_colors <- c("blue", "red", "green")
plot(pca_plot_data$PC1, pca_plot_data$PC2,
     main = "PCA of Training Data",
     xlab = "First Principal Component (PC1)",
     ylab = "Second Principal Component (PC2)",
     pch = 19,  # Use solid circles for points
     col = cluster_colors[true_cluster_ids]) # Assign colors based on true cluster IDs
legend("bottomright",
       legend = c("Cluster 1", "Cluster 2", "Cluster 3"),
       col = cluster_colors,
       pch = 19,
       title = "True Cluster")
```

## Learning Bernoulli mixture model using EM algorithm

The EM algorithm includes the following steps:

### Expectation (E) step

The goal of the **Expectation (E) step** is to calculate the "responsibilities" or weights for each data point. The weight $w_{im}$ represents the posterior probability that a given data point $\boldsymbol{x}_i$ was generated by cluster $m$, using the current estimates of the model parameters ($\pi$ and $\boldsymbol{\mu}$).

The formula for calculating the weight $w_{im}$ for the $i$-th data point and the $m$-th cluster is:

$$
w_{im} = \frac{\pi_m \prod_{d=1}^{D} \mu_{m,d}^{x_{i,d}} (1-\mu_{m,d})^{1-x_{i,d}}}{\sum_{k=1}^{M} \pi_k \prod_{d=1}^{D} \mu_{k,d}^{x_{i,d}} (1-\mu_{k,d})^{1-x_{i,d}}}
$$

Where:

* $w_{im}$ is the responsibility of cluster $m$ for data point $i$.
* $\pi_m$ is the current estimate of the mixing coefficient (prior probability) for cluster $m$.
* $\boldsymbol{\mu}_m = (\mu_{m,1}, ..., \mu_{m,D})$ is the vector of Bernoulli probabilities for cluster $m$.
* $\boldsymbol{x}_i = (x_{i,1}, ..., x_{i,D})$ is the $i$-th data point, which is a $D$-dimensional binary vector.
* The numerator is the joint probability of observing $\boldsymbol{x}_i$ from cluster $m$.
* The denominator is the sum of these joint probabilities over all possible clusters ($k=1, ..., M$), which serves to normalize the weights for each data point so they sum to 1.

### Calculating the Log-Likelihood

The log-likelihood is a critical measure used to assess how well the mixture model fits the entire dataset. In the EM algorithm, the objective is to maximize this value. A higher log-likelihood indicates that the current parameters make the observed data more probable.

For a dataset with $n$ data points $\boldsymbol{X} = \{\boldsymbol{x}_1, ..., \boldsymbol{x}_n\}$, the total log-likelihood, denoted as $\mathcal{L}(\theta | \boldsymbol{X})$, is the sum of the log-probabilities of each individual data point:

$$
\mathcal{L}(\theta | \boldsymbol{X}) = \sum_{i=1}^{n} \log \left( p(\boldsymbol{x}_i | \theta) \right)
$$

The probability of a single data point, $p(\boldsymbol{x}_i | \theta)$, is the marginal probability obtained by summing over all possible clusters.  For a Bernoulli mixture model, this is:

$$
p(\boldsymbol{x}_i | \theta) = \sum_{m=1}^{M} \pi_m \left( \prod_{d=1}^{D} \mu_{m,d}^{x_{i,d}} (1-\mu_{m,d})^{1-x_{i,d}} \right)
$$

Combining these gives the full log-likelihood formula that is calculated at each iteration of the EM algorithm:

$$
\mathcal{L}(\theta | \boldsymbol{X}) = \sum_{i=1}^{n} \log \left( \sum_{m=1}^{M} \pi_m \prod_{d=1}^{D} \mu_{m,d}^{x_{i,d}} (1-\mu_{m,d})^{1-x_{i,d}} \right)
$$

Where:

*   $\theta$ represents all the model parameters, which includes all mixing coefficients $\pi_m$ and all Bernoulli probability vectors $\boldsymbol{\mu}_m$.
*   $n$ is the total number of data points.
*   $M$ is the number of clusters.
*   $D$ is the dimensionality of the data.
*   $x_{i,d}$ is the value of the $d$-th dimension for the $i$-th data point.

### The M-Step Formulas

In the **Maximization (M) step**, the goal is to update the model parameters ($\pi$ and $\boldsymbol{\mu}$) to new values that maximize the expected log-likelihood, using the responsibilities ($w_{im}$) calculated in the E-step. These update rules are derived by taking the derivative of the expected log-likelihood function with respect to each parameter and setting it to zero.

#### 1. M-Step for Mixing Coefficients ($\pi$)

The mixing coefficient $\pi_m$ is updated to be the average responsibility that cluster $m$ takes over all data points. It represents the new estimate for the proportion of data points belonging to cluster $m$.

$$
\pi_m^{\text{new}} = \frac{1}{n} \sum_{i=1}^{n} w_{im}
$$

Where:

*   $w_{im}$ is the responsibility of cluster $m$ for data point $i$ (from the E-step).
*   $n$ is the total number of data points.

#### 2. M-Step for Conditional Probabilities ($\mu$)

The parameter $\mu_{m,d}$ (the probability of success for dimension $d$ in cluster $m$) is updated to be the weighted average of the values of the $d$-th dimension across all data points. The weight for each data point $x_{i,d}$ is its responsibility $w_{im}$ for that cluster.

$$
\mu_{m,d}^{\text{new}} = \frac{\sum_{i=1}^{n} w_{im} x_{i,d}}{\sum_{i=1}^{n} w_{im}}
$$

Where:

*   $x_{i,d}$ is the value of the $d$-th dimension of the $i$-th data point.
*   The denominator, $\sum_{i=1}^{n} w_{im}$, is the "effective" number of data points assigned to cluster $m$.

## Model fitting results

```{r task_2_functions}
em_algo <- function(M, x, seed=NULL) {
  n <- nrow(x)
  D <- ncol(x)
  max_it <- 100
  min_change <- 0.1
  w <- matrix(nrow=n, ncol=M) # weights
  pi <- vector(length = M) # mixing coefficients
  mu <- matrix(nrow=M, ncol=D) # conditional distributions
  llik <- vector(length = max_it) # log likelihood of the EM iterations
  
  # If a seed is provided, set it. This makes the run reproducible.
  # Important as the algorithm appears to be very sensitive to     initialization
  if (!is.null(seed)) {
    set.seed(seed)
  }
  # Random initialization of the parameters
  pi <- runif(M,0.49,0.51)
  pi <- pi / sum(pi)
  for(m in 1:M) {
    mu[m,] <- runif(D,0.49,0.51)
  }
  colors = c("blue", "red", "green", "yellow")
  for(it in 1:max_it) {
    # E-step: Computation of the weights
    for (i in 1:n) {
      # Calculate the probability of each data point belonging to each cluster
      for (m in 1:M) {
        # The term prod(mu[m,]^x[i,] * (1-mu[m,])^(1-x[i,])) calculates the probability
        # of observing the data point x[i,] given it was generated by cluster m.
        # This is multiplied by the mixing coefficient pi[m].
        w[i, m] <- pi[m] * prod(mu[m,]^x[i,] * (1 - mu[m,])^(1 - x[i,]))
      }
      # Normalize the weights for each data point so that they sum to 1.
      # This gives the posterior probability of cluster membership.
      w[i,] <- w[i,] / sum(w[i,])
    }
    #Log likelihood computation.
    llik[it] <- 0
    for (i in 1:n) {
      log_sum <- 0
      for (m in 1:M) {
        # The probability of a data point is the sum of the probabilities of that
        # data point being generated by each cluster, weighted by the mixing coefficients.
        log_sum <- log_sum + pi[m] * prod(mu[m,]^x[i,] * (1 - mu[m,])^(1 - x[i,]))
      }
      # We take the log of this sum and add it to the total log-likelihood.
      llik[it] <- llik[it] + log(log_sum)
    }
    flush.console()
    # Stop if the lok likelihood has not changed significantly
    if (it > 1) {
      if (abs(llik[it] - llik[it - 1]) < min_change) {
        break
      }
    }
    #M-step: ML parameter estimation from the data and weights
    # Re-estimate the mixing coefficients.
    for (m in 1:M) {
      # The new mixing coefficient for a cluster is the average of the weights
      # of that cluster over all data points.
      pi[m] <- sum(w[, m]) / n
    }
    # Re-estimate the conditional distributions (mu).
    for (m in 1:M) {
      # The new mean for a cluster is the weighted average of the data points,
      # where the weights are the posterior probabilities of cluster membership.
      mu[m,] <- colSums(x * w[, m]) / sum(w[, m])
    }
  }
  results <- list(
    pi = pi,                 # Final mixing coefficients
    mu = mu,                 # Final conditional probabilities (centroids)
    llik = llik[1:it]      # Log-likelihood values for iterations that were run
  )
  
  return(results)
}


plotting <- function(pi, mu, llik) {
  colors <- c("blue", "red", "green", "yellow")
  # par(mfrow = c(2, 1))
  plot(mu[1,], type="o", col="blue", ylim=c(0,1),
       main = "Final Cluster Centroids (mu)",
       xlab = "Dimension",
       ylab = "Probability")
  # grid()

  for (j in 2:nrow(mu)) {
    points(mu[j,], type="o", col=colors[j])
  }
  legend("bottomright", legend = paste("Cluster", 1:M), col = colors, lty = 1, pch = 1, cex=0.8)
  
  plot(llik[1:length(llik)], type="o",
       main = "Log-Likelihood Convergence",
       xlab = "Iteration",
       ylab = "Log-Likelihood")
  # grid()
  
  # par(mfrow = c(1, 1))
}

run_and_plot <- function(M, x, seed_value) {
  results <- em_algo(M, x, seed_value)
  pi <- results$pi
  mu <- results$mu
  llik <- results$llik
  print(paste("Number of clusters: ", M))
  print(paste("Log likelihood: ", llik[length(llik)]))
  print(paste("pi: ", paste(pi, collapse=", ")))
  plotting(pi, mu, llik)
  return(results)
}

all_models <- list()
```

### Fitting results for 2 clusters

```{r task_2_fitting_2_clusters}
M=2 # number of clusters
all_models[[as.character(M)]] <- run_and_plot(M, x, seed_value)
```

### Fitting results for 3 clusters

```{r task_2_fitting_3_clusters}
M=3 # number of clusters
all_models[[as.character(M)]] <- run_and_plot(M, x, seed_value)
```

### Fitting results for 4 clusters

```{r task_2_fitting_4_clusters}
M=4 # number of clusters
all_models[[as.character(M)]] <- run_and_plot(M, x, seed_value)
```

### Analysis

The above plots do not necessarily provide good insights into which number of clusters gives the best fit to the training data. We can say that the model with 3 clusters gives the best fit because the $\mu$ plot is similar to the original one we used for generating the data but in real life we won't have access to it. Therefore we need more objective criteria for model selection.

**Akaike Information Criterion (AIC)** and **Bayesian Information Criterion (BIC)** are standard tools for model selection. They balance model fit (log-likelihood) with model complexity (number of parameters). The model with the lowest AIC or BIC is considered the best.

```{r task_2_result_comparison}
results_table <- data.frame(
  M = integer(),
  LogLikelihood = double(),
  NumParams = integer(),
  AIC = double(),
  BIC = double()
)

for (M in 2:4) {
  model <- all_models[[as.character(M)]]
  
  logL <- tail(model$llik, 1)
  k <- (M - 1) + (M * D)
  
  aic <- 2 * k - 2 * logL
  bic <- k * log(n) - 2 * logL
  
  results_table <- rbind(results_table, data.frame(
    M = M,
    LogLikelihood = logL,
    NumParams = k,
    AIC = aic,
    BIC = bic
  ))
}

knitr::kable(results_table, digits = 2, caption = "Model Selection Results")
```

Based on the results above, the model with 3 clusters provides the best fit as it has the lowest AIC and BIC compared to the models with 2 or 4 clusters.


# Assignment 3.

# Assignment 4. Theory questions.

**Q2 In AdaBoost, what is the loss function used to train the boosted classifier at each iteration?** Page 177 lists the loss function used by the AdaBoost classifier as follows:
$$
L(y \cdot f(\mathbf{x})) = \exp(-y \cdot f(\mathbf{x}))
$$
where:

- $y$ is the true label for the instance, typically $+1$ or $-1$
- $f(\mathbf{x})$ is the current margin output from the ensemble classifier for the data point $\mathbf{x}$
The product $y \cdot f(\mathbf{x})$ is known as the margin and indicates how confidently the classifier predicts the correct label.


## Appendix

All code

```{r, ref.label=setdiff(knitr::all_labels(), c('setup')), echo=TRUE, eval=FALSE}
```
